{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb0bb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers datasets torch accelerate bitsandbytes peft\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Check model's max context length\n",
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-3B\", trust_remote_code=True)\n",
    "print(f\"Model's max context length: {config.max_position_embeddings}\")\n",
    "\n",
    "def prepare_dataset(tokenizer):\n",
    "    \"\"\"\n",
    "    Loads a dataset and formats it with proper instruction tuning format\n",
    "    \"\"\"\n",
    "    print(\"Loading and preparing dataset...\")\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "\n",
    "    # Select 1000 examples first\n",
    "    small_dataset = dataset['train'].select(range(1000))\n",
    "    \n",
    "    def format_instruction(example):\n",
    "        # Format the instruction and response\n",
    "        if example[\"input\"]:\n",
    "            instruction = (\n",
    "                f\"{tokenizer.bos_token}### Instruction: {example['instruction']}\\n\"\n",
    "                f\"### Input: {example['input']}\\n\"\n",
    "                f\"### Response:\"\n",
    "            )\n",
    "            response = f\" {example['output']}{tokenizer.eos_token}\"\n",
    "        else:\n",
    "            instruction = (\n",
    "                f\"{tokenizer.bos_token}### Instruction: {example['instruction']}\\n\"\n",
    "                f\"### Response:\"\n",
    "            )\n",
    "            response = f\" {example['output']}{tokenizer.eos_token}\"\n",
    "        \n",
    "        # Tokenize separately\n",
    "        prompt_ids = tokenizer(instruction, add_special_tokens=False)\n",
    "        response_ids = tokenizer(response, add_special_tokens=False)\n",
    "        \n",
    "        # Combine and create labels\n",
    "        input_ids = prompt_ids[\"input_ids\"] + response_ids[\"input_ids\"]\n",
    "        attention_mask = [1] * len(input_ids)  # All tokens should be attended to\n",
    "        labels = [-100] * len(prompt_ids[\"input_ids\"]) + response_ids[\"input_ids\"]\n",
    "        \n",
    "        # Truncate if too long \n",
    "        max_length = 1024\n",
    "        if len(input_ids) > max_length:\n",
    "            input_ids = input_ids[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "            labels = labels[:max_length]\n",
    "            \n",
    "        # Pad if needed\n",
    "        while len(input_ids) < max_length:\n",
    "            input_ids.append(tokenizer.pad_token_id)\n",
    "            attention_mask.append(0)\n",
    "            labels.append(-100)\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    print(\"Formatting dataset...\")\n",
    "    formatted_dataset = small_dataset.map(\n",
    "        format_instruction,\n",
    "        remove_columns=dataset.column_names[\"train\"]\n",
    "    )\n",
    "    \n",
    "    small_dataset = formatted_dataset['train'].select(range(1000))\n",
    "    print(f\"Dataset prepared with {len(small_dataset)} examples\")\n",
    "    return small_dataset\n",
    "\n",
    "def prepare_fine_tuning():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"./qwen_instruct_{timestamp}\"\n",
    "    \n",
    "    print(\"Starting fine-tuning preparation...\")\n",
    "    \n",
    "    model_id = \"Qwen/Qwen2.5-3B\"\n",
    "    print(f\"Loading tokenizer from {model_id}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    \n",
    "    print(f\"Loading model from {model_id} with 4-bit quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Add LoRA configuration\n",
    "    print(\"Applying LoRA adapters...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    tokenized_dataset = prepare_dataset(tokenizer)\n",
    "\n",
    "    print(\"Setting up training arguments...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=10,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=True,\n",
    "        warmup_steps=50,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # print(\"Initializing trainer...\")\n",
    "    # trainer = Trainer(\n",
    "    #     model=model,\n",
    "    #     args=training_args,\n",
    "    #     train_dataset=tokenized_dataset,\n",
    "    #     data_collator=DataCollatorForLanguageModeling(\n",
    "    #         tokenizer=tokenizer, \n",
    "    #         mlm=False\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    print(\"Initializing trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        # Remove DataCollator as we're handling formatting in dataset\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    final_output_dir = f\"{output_dir}_final\"\n",
    "    print(f\"Saving final model to {final_output_dir}\")\n",
    "    trainer.save_model(final_output_dir)\n",
    "    print(\"Fine-tuning completed!\")\n",
    "\n",
    "prepare_fine_tuning()\n",
    "\n",
    "def test_model(instruction, input_text=None):\n",
    "    # Load the base model config first\n",
    "    base_model_id = \"Qwen/Qwen2.5-3B\"\n",
    "    lora_path = \"/content/qwen_instruct_20250123_041812_final\"\n",
    "    \n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    # Load tokenizer from base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "    \n",
    "    # Load base model with config\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapters\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(model, lora_path)\n",
    "    \n",
    "    # Format the prompt exactly like training data\n",
    "    if input_text:\n",
    "        prompt = (\n",
    "            f\"{tokenizer.bos_token}### Instruction: {instruction}\\n\"\n",
    "            f\"### Input: {input_text}\\n\"\n",
    "            f\"### Response:\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"{tokenizer.bos_token}### Instruction: {instruction}\\n\"\n",
    "            f\"### Response:\"\n",
    "        )\n",
    "    \n",
    "    print(\"\\nPrompt:\", prompt)\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,  # Enable sampling for temperature to work\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Clean up response - only show after \"### Response:\"\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    print(\"\\nModel Response:\", response)\n",
    "\n",
    "\n",
    "# Test 1: Simple instruction\n",
    "print(\"\\nTest 1: Simple instruction\")\n",
    "test_model(\"Write a haiku about programming\")\n",
    "\n",
    "# Test 2: Translation with input\n",
    "print(\"\\nTest 2: Translation task\")\n",
    "test_model(\"Translate this to French\", \"Hello, how are you?\")\n",
    "\n",
    "# Test 3: Complex explanation\n",
    "print(\"\\nTest 3: Explanation task\")\n",
    "test_model(\"Explain the concept of recursion to a 5 year old\")\n",
    "\n",
    "# Test 4: Creative writing\n",
    "print(\"\\nTest 4: Creative task\")\n",
    "test_model(\"Write a short story about a robot learning to paint\")\n",
    "\n",
    "# Test 5: Problem solving\n",
    "print(\"\\nTest 5: Problem solving\")\n",
    "test_model(\"How would you solve this problem?\", \"I need to sort a list of numbers efficiently\")\n",
    "\n",
    "# First, zip the model files\n",
    "!zip -r /content/my_model.zip /content/qwen_instruct_20250123_041812_final\n",
    "\n",
    "# Download to local machine\n",
    "from google.colab import files\n",
    "files.download('/content/my_model.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
