{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers datasets torch accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503efc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import torch\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c565177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenizer):\n",
    "    \"\"\"\n",
    "    Loads a dataset and formats each example into an instruction-based string, then tokenizes it.\n",
    "    Returns the tokenized dataset for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading and preparing dataset...\")\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "    \n",
    "    def format_instruction(example):\n",
    "        if example[\"input\"]:\n",
    "            instruction = (\n",
    "                f\"### Instruction: {example['instruction']}\\n\"\n",
    "                f\"### Input: {example['input']}\\n\"\n",
    "                f\"### Response: {example['output']}\"\n",
    "            )\n",
    "        else:\n",
    "            instruction = (\n",
    "                f\"### Instruction: {example['instruction']}\\n\"\n",
    "                f\"### Response: {example['output']}\"\n",
    "            )\n",
    "        return {\"text\": instruction}\n",
    "\n",
    "    print(\"Formatting dataset...\")\n",
    "    formatted_dataset = dataset.map(format_instruction)\n",
    "    \n",
    "    small_dataset = formatted_dataset['train'].select(range(1000))\n",
    "    \n",
    "    print(\"Tokenizing dataset...\")\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = small_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=small_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset prepared with {len(tokenized_dataset)} examples\")\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fine_tuning():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"./qwen_instruct_{timestamp}\"\n",
    "    \n",
    "    print(\"Starting fine-tuning preparation...\")\n",
    "    \n",
    "    model_id = \"Qwen/Qwen2.5-3B\"\n",
    "    print(f\"Loading tokenizer from {model_id}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    \n",
    "    print(f\"Loading model from {model_id} with 4-bit quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    tokenized_dataset = prepare_dataset(tokenizer)\n",
    "\n",
    "    print(\"Setting up training arguments...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=10,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=True,\n",
    "        warmup_steps=50,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    print(\"Initializing trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, \n",
    "            mlm=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    final_output_dir = f\"{output_dir}_final\"\n",
    "    print(f\"Saving final model to {final_output_dir}\")\n",
    "    trainer.save_model(final_output_dir)\n",
    "    print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f70799",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_fine_tuning()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
