{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes  # Upgrade bitsandbytes to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback\n",
    ")\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Custom callback to log additional information.\n",
    "class CustomLoggingCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Print out a message at the end of each epoch.\n",
    "        # state.log_history is a list of logged metrics.\n",
    "        last_log = state.log_history[-1] if state.log_history else {}\n",
    "        print(f\"\\n>> Epoch {state.epoch:.2f} completed. Logged Metrics: {last_log}\")\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_pgn_dataset(tokenizer):\n",
    "    \"\"\"\n",
    "    Loads a chess PGN dataset and tokenizes it.\n",
    "\n",
    "    Expected:\n",
    "      - The dataset contains examples with a key \"pgn\" for raw PGN moves.\n",
    "      - Replace \"your_chess_pgn_dataset\" with your actual dataset identifier.\n",
    "\n",
    "    Returns the tokenized dataset.\n",
    "    \"\"\"\n",
    "    print(\"Loading chess PGN dataset...\")\n",
    "    dataset = load_dataset(\"parquet\", data_files=\"/content/drive/MyDrive/Colab_Data/data/chess_pgn_dataset_10k.parquet\")  # path to drive\n",
    "\n",
    "    def format_pgn(example):\n",
    "        # Remap the PGN string to the \"text\" field for tokenization.\n",
    "        return {\"text\": example[\"pgn\"]}\n",
    "\n",
    "    # Format the dataset with our PGN string.\n",
    "    dataset = dataset.map(format_pgn)\n",
    "\n",
    "    # For quick experiments, select a small subset.\n",
    "    if \"train\" in dataset:\n",
    "        print(\"train in dataset\")\n",
    "        small_dataset = dataset[\"train\"].select(range(4000))\n",
    "    else:\n",
    "        print(\"no train in dataset\")\n",
    "        small_dataset = dataset.select(range(5000))\n",
    "\n",
    "    # Print the first 5 examples\n",
    "    for i in range(5):\n",
    "        print(small_dataset[i])\n",
    "\n",
    "    # Ensure the tokenizer has a pad token. Option 1: Set pad token to eos_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Alternatively, to add a new pad token:\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    print(\"Tokenizing the PGN dataset...\")\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=512,  # We keep the start of the PGN; anything beyond 512 tokens is truncated.\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = small_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=small_dataset.column_names\n",
    "    )\n",
    "\n",
    "    print(f\"Prepared tokenized dataset with {len(tokenized_dataset)} examples.\")\n",
    "    return tokenized_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_pgn_finetuning():\n",
    "    \"\"\"\n",
    "    Sets up and runs PGN-based domain-adaptive fine-tuning.\n",
    "\n",
    "    Key Points:\n",
    "      - Uses raw PGN strings for next-token prediction (i.e. domain adaptation).\n",
    "      - Uses a lower learning rate (1e-5) for gentle, full-model updates.\n",
    "      - Designed to update the model's entire latent space (hence no PEFT/LoRA).\n",
    "\n",
    "    The final model and checkpoints will be saved to your Google Drive.\n",
    "    \"\"\"\n",
    "    # Create a structured output directory in Google Drive.\n",
    "    base_dir = \"/content/drive/MyDrive/Colab_Data/fine_tuned_models/chess_pgn_finetuning\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(base_dir, f\"run_{timestamp}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "\n",
    "    print(\"Loading instruct model and tokenizer for chess PGN fine-tuning...\")\n",
    "    # Use the instruct model that has shown better PGN prediction performance.\n",
    "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    # Define quantization configuration\n",
    "    # quant_config = BitsAndBytesConfig(\n",
    "    #    load_in_4bit=True,\n",
    "    #    bnb_4bit_quant_type=\"nf4\",  # or \"fp4\" depending on your preference\n",
    "    #    bnb_4bit_use_double_quant=True,\n",
    "    #    bnb_4bit_compute_dtype=torch.float16\n",
    "    # )\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",   # Automatically allocate layers to available GPU(s) or CPU.\n",
    "        # quantization_config=quant_config,\n",
    "        # torch_dtype=torch.float16  # Mixed-precision training. # autocast auto handles these conversions\n",
    "    )\n",
    "\n",
    "    # Prepare the tokenized PGN dataset.\n",
    "    tokenized_dataset = prepare_pgn_dataset(tokenizer)\n",
    "\n",
    "\n",
    "    # Split the tokenized dataset into train (90%) and eval (10%) subsets\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Configuring training arguments...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,            # Directory to store model checkpoints.\n",
    "        num_train_epochs=5,               # Number of training epochs.\n",
    "        per_device_train_batch_size=4,    # Batch size per device.\n",
    "        gradient_accumulation_steps=4,    # To simulate a larger batch size.\n",
    "        save_steps=500,                   # Save checkpoints every 500 steps.\n",
    "        save_total_limit=2,               # Only save the last 2 checkpoints.\n",
    "        logging_steps=10,                 # Log training metrics every 10 steps.\n",
    "        learning_rate=1e-5,               # Lower learning rate for gentle fine-tuning.\n",
    "        fp16=True,                        # Use 16-bit precision.\n",
    "        warmup_steps=50,                  # Warmup steps.\n",
    "        report_to=\"none\",                 # Disable external logging.\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,                   # Evaluate every 100 steps.\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False  # Causal language modeling (next-token prediction).\n",
    "        ),\n",
    "        callbacks=[CustomLoggingCallback()]\n",
    "    )\n",
    "\n",
    "    print(\"Starting PGN fine-tuning...\")\n",
    "    trainer.train()\n",
    "\n",
    "    final_output_dir = f\"{output_dir}_final\"\n",
    "    print(f\"Saving final model to {final_output_dir}...\")\n",
    "    trainer.save_model(final_output_dir)\n",
    "    print(\"PGN fine-tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the fine-tuning process.\n",
    "prepare_pgn_finetuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
